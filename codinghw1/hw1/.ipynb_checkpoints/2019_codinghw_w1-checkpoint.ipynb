{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.chdir('flowers17feats/flowers17/feats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading\n",
    "\n",
    "append the .npy arrays into list of list (17, 80, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_list = []\n",
    "current_list = []\n",
    "count = 0\n",
    "\n",
    "for filename in os.listdir(os.getcwd()):\n",
    "    current_list.append(np.load(filename))\n",
    "    count += 1\n",
    "    if count == 80:\n",
    "        master_list.append(current_list)\n",
    "        current_list = []\n",
    "        count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, validation, test split\n",
    "\n",
    "split the master_list into test, validation and training sets. save into np arrays (680,512), (340,512), (340,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_list = []\n",
    "validation_list = []\n",
    "test_list = []\n",
    "\n",
    "for listdata in master_list:\n",
    "    random.shuffle(listdata)\n",
    "    training_set, validation_set, test_set = np.array(listdata[0:40]), np.array(listdata[40:60]), np.array(listdata[60:80])\n",
    "    train_list.append(training_set)\n",
    "    validation_list.append(validation_set)\n",
    "    test_list.append(test_set)\n",
    "\n",
    "train_list = np.array(np.vstack(np.stack(train_list))) #(680,512)\n",
    "validation_list = np.array(np.vstack(np.stack(validation_list))) #340,512\n",
    "test_list = np.array(np.vstack(np.stack(test_list))) #340,512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why split evenly amongst the classes\n",
    "\n",
    "This is to ensure training and testing are on the same distribution since the split is evenly distributed amongst all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"train_data.npy\", train_list)\n",
    "np.save(\"validation_data.npy\", validation_list)\n",
    "np.save(\"test_data.npy\", test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training code\n",
    "\n",
    "Create 17 binary svms trained on the training data. For each of the class, set a np array (680,) of zeros with the particular class set as ones. Use probability = True for SVM to output prediction score. Append the classifiers into a clf_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm as svm\n",
    "\n",
    "\n",
    "def train_clf(c, train_data):\n",
    "\n",
    "    clf_list = []\n",
    "    batch = train_data.shape[0]//17\n",
    "    \n",
    "    for x in range(17):\n",
    "        train_label = np.zeros((train_data.shape[0],))\n",
    "        train_label[x*batch:(x+1)*batch] += 1\n",
    "        clf = svm.SVC(C=c,probability = True, kernel='linear') #kernel = linear is equivalent to linear svm\n",
    "        clf.fit(train_data, train_label)\n",
    "        clf_list.append(clf)\n",
    "    \n",
    "    return clf_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing code\n",
    "\n",
    "For all data in test_data, pass it through the 17 svms and see which one outputs the highest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_clf(test_data, clf_list):\n",
    "    \n",
    "    correct_pred = 0\n",
    "    errors = []\n",
    "    batch = test_data.shape[0]//17\n",
    "\n",
    "    for count,datapoint in enumerate(test_data):\n",
    "        label = count // batch\n",
    "        datapoint = np.reshape(datapoint, (1,-1))\n",
    "        cmp_list = []\n",
    "        for clf in clf_list:\n",
    "            prob = clf.predict_proba(datapoint)\n",
    "            cmp_list.append(prob[0][1])\n",
    "        pred = np.argmax(np.array(cmp_list))\n",
    "\n",
    "        correct_pred += (pred==label)\n",
    "        if pred != label:\n",
    "            errors.append(count)\n",
    "        \n",
    "    return (correct_pred/test_data.shape[0]), errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the best c\n",
    "\n",
    "Testing for best c in validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = [0.01, 0.1, 0.1**0.5, 1, 10**0.5, 10, 100**0.5]\n",
    "accuracy_list = []\n",
    "\n",
    "for c in c_values:\n",
    "    clf_list = train_clf(c, train_list)\n",
    "    accuracy, errors = test_clf(validation_list, clf_list)\n",
    "    accuracy_list.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8941176470588236,\n",
       " 0.8911764705882353,\n",
       " 0.9,\n",
       " 0.8970588235294118,\n",
       " 0.9,\n",
       " 0.9,\n",
       " 0.9]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(c_values, accuracy_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best c is apparently 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on training and validation data with best c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = np.split(train_list, 17)\n",
    "validation_list = np.split(validation_list, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1020, 512)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_list = np.array(np.vstack([np.concatenate(list(a)) for a in zip(train_list, validation_list)]))\n",
    "new_train_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_list = train_clf(0.01, new_train_list)\n",
    "accuracy, errors = test_clf(test_list, clf_list)\n",
    "error_dict = {}\n",
    "for error in errors:\n",
    "    error_class = error // 40\n",
    "    error_dict[error_class] = error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (accuracy, error_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
